Solution 1
class LinearRegression:
    def __init__(self, learning_rate,epochs,penalty='None',alpha=0):
        self.lr=learning_rate
        self.epochs=epochs
        self.penalty = penalty
        self.alpha=alpha
        
    def fit(self, X_train, y_train):
        n_samples, n_features = X_train.shape
        y_train=y_train.reshape(-1,1)
        # init parameters
        self.weights = np.zeros((n_features,1))
        self.bias = np.zeros((1,1))
        
        # gradient descent
        for i in range(self.epochs):
            delta= -2*(y_train-np.dot(X_train,self.weights)-self.bias)/n_samples
            dw=0
            if self.penalty == 'l1':
                dw= np.dot(X_train.T,delta)+ self.alpha
            elif self.penalty == 'l2':
                dw= np.dot(X_train.T,delta) + 2*self.alpha*np.sum(self.weights)
            else:
                dw= np.dot(X_train.T,delta)
            
            db= np.sum(delta).reshape(1,1)

            #update weights and biases
            self.weights-= self.lr * dw
            self.bias-= self.lr* db

    def predict(self, X_test):
        y_predicted = np.dot(X_test,self.weights)+self.bias
        return y_predicted


Solution 2
import numpy as np
import pandas as pd

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

df = pd.read_csv("/kaggle/input/real-estate-price-prediction/Real estate.csv")
df.info()

df.head()

cols = list(df.columns)
cols.remove('No')
cols.remove('Y house price of unit area')
cols.remove('X1 transaction date')
cols

import matplotlib.pyplot as plt
i=1
for col in cols:
    plt.subplot(2,3,i)
    i+=1
    plt.scatter(df[col],df['Y house price of unit area'])

X=df[cols]
y=df['Y house price of unit area']
from sklearn.model_selection import train_test_split as tts
X_train,X_test,y_train,y_test = tts(X,y,test_size=0.25,random_state=10)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

X_train=np.array(X_train)
X_test=np.array(X_test)
y_train=np.array(y_train)
y_test=np.array(y_test)

from sklearn.preprocessing import MinMaxScaler
scaler= MinMaxScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)

reg1= LinearRegression(learning_rate=0.01,epochs=3000)
reg2= LinearRegression(learning_rate=0.01,epochs=3000,penalty='l1',alpha=0.015)
reg3= LinearRegression(learning_rate=0.01,epochs=3000,penalty='l2',alpha=0.015)

reg1.fit(X_train,y_train)
reg2.fit(X_train,y_train)
reg3.fit(X_train,y_train)

from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import r2_score

y_pred=reg1.predict(X_test)
print(mse(y_test,y_pred))
print(r2_score(y_test,y_pred))

y_pred=reg2.predict(X_test)
print(mse(y_test,y_pred))
print(r2_score(y_test,y_pred))

y_pred=reg3.predict(X_test)
print(mse(y_test,y_pred))
print(r2_score(y_test,y_pred))

from sklearn.linear_model import LinearRegression as Linreg,Lasso,Ridge

reg1= Linreg()
reg2= Lasso()
reg3= Ridge()

reg1.fit(X_train,y_train)
reg2.fit(X_train,y_train)
reg3.fit(X_train,y_train)

y_pred=reg1.predict(X_test)
print(mse(y_test,y_pred))
print(r2_score(y_test,y_pred))

y_pred=reg2.predict(X_test)
print(mse(y_test,y_pred))
print(r2_score(y_test,y_pred))

y_pred=reg3.predict(X_test)
print(mse(y_test,y_pred))
print(r2_score(y_test,y_pred))



Solution 3
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()
Y=data.target
X = data.data
print(X.shape)
print(Y.shape)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X=scaler.fit_transform(X)

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.25)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

import numpy as np
def sigmoid(z):
    a=1.0/(1.0+ np.exp(-z))
    return a
def sigmoid_derivative(z):
    return sigmoid(z)*(1-sigmoid(z))

class LogisticRegression:
    def __init__(self, learning_rate,epochs,penalty='None',alpha=0):
        self.lr=learning_rate
        self.epochs=epochs
        self.penalty = penalty
        self.alpha=alpha

    def fit(self, X, y):
        n_samples, n_features = X.shape
        y = y.reshape(-1, 1)
        self.weights=np.random.randn(n_features,1)/np.sqrt(n_features)
        self.bias= np.random.randn(1,1)

        for i in range(self.epochs):
            z = np.dot(X,self.weights) + self.bias
            y_pred = sigmoid(z)

            
            dw = -np.dot(X.T,(y - y_pred))/n_samples

            if self.penalty == 'l1':
                dw += self.alpha
            elif self.penalty == 'l2':
                dw += 2*self.alpha*np.sum(self.weights)
                
            db = -np.sum(y - y_pred)/n_samples
            self.weights -= self.lr* dw
            self.bias-= self.lr* db

    def predict(self, X):
        y_pred = np.dot(X,self.weights)+self.bias
        Y_proba_pred=y_pred
        for i in range(len(y_pred)):
            if y_pred[i]<= 0.5:
                y_pred[i] = 0
            else:
                y_pred[i] = 1   
        return (Y_proba_pred,y_pred)

# Create an instance of LogisticRegression
logreg1 = LogisticRegression(0.1,3000) #Play around with different learning rates and epochs
logreg2 = LogisticRegression(0.1,3000,penalty='l1',alpha=0.01)
logreg3 = LogisticRegression(0.1,3000,penalty='l1',alpha=0.01)

# Train the model
logreg1.fit(X_train,y_train)
logreg2.fit(X_train,y_train)
logreg3.fit(X_train,y_train)

# Make predictions on the test set
Y_proba_pred1,y_pred1 = logreg1.predict(X_test)
Y_proba_pred2,y_pred2 = logreg2.predict(X_test)
Y_proba_pred3,y_pred3 = logreg3.predict(X_test)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test,y_pred1))
print(accuracy_score(y_test,y_pred2))
print(accuracy_score(y_test,y_pred3))

from sklearn.linear_model import LogisticRegression as LR
logreg4 = LR(penalty=None)
logreg5 = LR(penalty='l1',solver='saga')
logreg6 = LR(penalty="l2")
logreg7 = LR(penalty="elasticnet",l1_ratio=0.4,solver='saga')

logreg4.fit(X_train,y_train)
logreg5.fit(X_train,y_train)
logreg6.fit(X_train,y_train)
logreg7.fit(X_train,y_train)

y_pred4 = logreg4.predict(X_test)
y_pred5 = logreg5.predict(X_test)
y_pred6 = logreg6.predict(X_test)
y_pred7 = logreg6.predict(X_test)

print(accuracy_score(y_test,y_pred4))
print(accuracy_score(y_test,y_pred5))
print(accuracy_score(y_test,y_pred6))
print(accuracy_score(y_test,y_pred7))

from sklearn.metrics import classification_report
target_names = data.target_names
print(classification_report(y_test, y_pred1, target_names=target_names))
print(classification_report(y_test, y_pred5, target_names=target_names))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_pred1))
print(confusion_matrix(y_test, y_pred5))

from sklearn.metrics import roc_auc_score
print(roc_auc_score(y_test, logreg5.predict_proba(X_test)[:, 1]))
print(roc_auc_score(y_test, Y_proba_pred2))

from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt
plt.figure(figsize=(6,6))
fpr1, tpr1, _ = roc_curve(y_test, logreg5.predict_proba(X_test)[:, 1], pos_label=1)
fpr2, tpr2, _= roc_curve(y_test, Y_proba_pred2, pos_label=1)
plt.plot(fpr1,tpr1,color='red')
plt.plot(fpr2,tpr2,color='blue')
plt.show()



Solution 4
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap as lcm

def euclidian_distance(x,x1):
  return np.sqrt(np.sum(np.subtract(x,x1)**2))

class KNN:
    def __init__(self,k):
        self.k=k
    def fit(self,X_train,y_train):
        self.X_train=X_train
        self.y_train=y_train
    def predict(self,X_test):
        y_pred=[]
        for x in X_test:
            distance = [euclidian_distance(x,x1) for x1 in self.X_train]
            k_shortest_indices = np.argsort(distance)[:self.k]
            k_labels = [self.y_train[i] for i in k_shortest_indices]
            common_label = np.bincount(k_labels)
            y_pred.append(np.argmax(common_label))
        return np.array(y_pred)
    
def accuracy(predictions,y_test):
    return np.sum(predictions==y_test)/len(y_test)

import pandas as pd
data= pd.read_csv('/content/glass.csv')
data.head()

x= data.drop('Type',axis=1)
y= data['Type']
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)

x_train = np.array(x_train)
y_train = np.array(y_train)
x_test = np.array(x_test)
y_test = np.array(y_test)

clf=KNN(k=3)
clf.fit(x_train,y_train)
predictions=clf.predict(x_test)
print(accuracy(predictions,y_test))

from sklearn.neighbors import KNeighborsClassifier as knn
clf = knn(n_neighbors=3)
clf.fit(x_train,y_train)
pred=clf.predict(x_test)
from sklearn.metrics import accuracy_score
accuracy_score(y_test,pred)

from sklearn.tree import DecisionTreeClassifier as dtc,plot_tree
clf=dtc(max_depth=10,criterion="entropy")
clf.fit(x_train,y_train)
pred=clf.predict(x_test)
from sklearn.metrics import accuracy_score
accuracy_score(y_test,pred)


plot_tree(clf,max_depth=5)
